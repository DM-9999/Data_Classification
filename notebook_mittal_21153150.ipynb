{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 831,
   "id": "66d7f71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn import tree\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "con = sqlite3.connect('Assignment2023.sqlite')\n",
    "train_df = pd.read_sql(\"SELECT * FROM train\", con, index_col= \"index\")\n",
    "test_df= pd.read_sql(\"SELECT * FROM test\", con, index_col= \"index\")\n",
    "\n",
    "# to use the index column instead of pandas index\n",
    "\n",
    "print(train_df.head())\n",
    "print()\n",
    "print()\n",
    "print(test_df.head())\n",
    "\n",
    "print(train_df.describe())      #to get desciption of all cols \n",
    "print(test_df.describe())\n",
    "\n",
    "train_df.hist(figsize=(11,9))\n",
    "plt.show()\n",
    "\n",
    "print(train_df[\"Att14\"].describe())\n",
    "\n",
    "corr= train_df[\"Att20\"].corr(train_df[\"Att27\"])\n",
    "\n",
    "\n",
    "print(train_df.dtypes)\n",
    "\n",
    "print(train_df[[\"Att11\", \"Att16\", \"Att25\"]])\n",
    "\n",
    "numeric_columns = train_df.select_dtypes(include=['number'])\n",
    "\n",
    "sns.heatmap(numeric_columns.corr(), vmax= 1, square = True)\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(train_df[\"Att00\"], train_df[\"Att20\"])\n",
    "plt.xlabel(\"Att00\")\n",
    "plt.ylabel(\"Att20\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(train_df[\"Att03\"], train_df[\"Att18\"])\n",
    "plt.xlabel(\"Att03\")\n",
    "plt.ylabel(\"Att18\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(train_df[\"Att06\"], train_df[\"Att23\"])\n",
    "plt.xlabel(\"Att06\")\n",
    "plt.ylabel(\"Att23\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(train_df[\"Att09\"], train_df[\"Att14\"])\n",
    "plt.xlabel(\"Att09\")\n",
    "plt.ylabel(\"Att14\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(train_df[\"Att13\"], train_df[\"Att15\"])\n",
    "plt.xlabel(\"Att13\")\n",
    "plt.ylabel(\"Att15\")\n",
    "plt.show()\n",
    "\n",
    "train_df.drop(columns=[\"Att00\",\"Att13\",\"Att03\",\"Att06\"], inplace= True) #showing high correaltion. Hence, dimension reduction is done\n",
    "\n",
    "print(train_df.shape)\n",
    "\n",
    "\n",
    "test_df.drop(columns=[\"Att00\",\"Att13\",\"Att03\",\"Att06\"], inplace =True)\n",
    "print(test_df.shape)\n",
    "\n",
    "\n",
    "\n",
    "numeric_columns = train_df.select_dtypes(include=['number'])\n",
    "\n",
    "sns.heatmap(numeric_columns.corr(), vmax= 1, square = True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_df['Att11'].value_counts().plot(kind='bar')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "train_df['Att16'].value_counts().plot(kind='bar')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "train_df['Att26'].value_counts().plot(kind='bar')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# tast2)\n",
    "\n",
    "train_df.isna().sum()\n",
    "\n",
    "def missing(df):\n",
    "    missing_dict= dict()\n",
    "    total= df.shape[0]\n",
    "    for att in df.columns:\n",
    "        missing= df[att].isna().sum()\n",
    "        frac= missing/total*100\n",
    "        missing_dict[att]= frac\n",
    "    return missing_dict\n",
    "m_dict= missing(train_df)\n",
    "cols_to_drop= [att for att, frac in m_dict.items() if frac>10]\n",
    "print(cols_to_drop)\n",
    "\n",
    "## its shows that Att24 have more than 50%  and Att15 about 10% missing values.\n",
    "\n",
    "\n",
    "train_df.drop(columns=[\"Att24\"], inplace= True)\n",
    "test_df.drop(columns=[\"Att24\"], inplace = True)\n",
    "\n",
    "train_df.isna().sum()\n",
    "\n",
    "\n",
    "cols_to_impute = [ att for att,frac in m_dict.items() if 0<frac <20]\n",
    "cols_to_impute\n",
    "for col in cols_to_impute:\n",
    "  # compute the mean\n",
    "  mean = test_df[col].mean()\n",
    "  # now use the fillna function to replace the NaN avalues with the mean value\n",
    "  test_df[col].fillna(mean, inplace=True)\n",
    "m_dict = missing(test_df)\n",
    "for col in cols_to_impute:\n",
    "  print(col, \"missing data\", m_dict[col])\n",
    "\n",
    "cols_to_impute = [ att for att,frac in m_dict.items() if 0<frac <20]\n",
    "cols_to_impute\n",
    "for col in cols_to_impute:\n",
    "  # compute the mean\n",
    "  mean = train_df[col].mean()\n",
    "  # now use the fillna function to replace the NaN avalues with the mean value\n",
    "  train_df[col].fillna(mean, inplace=True)\n",
    "m_dict = missing(train_df)\n",
    "for col in cols_to_impute:\n",
    "  print(col, \"missing data\", m_dict[col])\n",
    "\n",
    "trainNA = train_df.isna().sum()\n",
    "trainNA[trainNA != 0]\n",
    "\n",
    "# 3)) to handle and remove all the duplicate values\n",
    "\n",
    "\n",
    "\n",
    "train_df.duplicated().sum()\n",
    "test_df.duplicated().sum()\n",
    "\n",
    "# Hence, there is no dupliacte rows in this dataset.\n",
    "\n",
    "# 4))select suitable datatype of for each attribute    \n",
    "\n",
    "train_df.head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Display the DataFrame with all columns\n",
    "print(train_df.dtypes)\n",
    "\n",
    "train_df_grouped = train_df.melt(value_vars=['Att11', 'Att16', 'Att25'], value_name='Category')\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.countplot(x='variable', hue='Category', data=train_df_grouped)\n",
    "plt.xlabel('Categorical Columns')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Count of each category in different columns')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Categories', bbox_to_anchor=(1, 1))\n",
    "plt.show()\n",
    "\n",
    "train_df['Att04'] = train_df.Att01.astype('category').cat.codes\n",
    "train_df['Att11'] = train_df.Att11.astype('category').cat.codes\n",
    "train_df['Att16'] = train_df.Att16.astype('category').cat.codes\n",
    "train_df['Att25'] = train_df.Att25.astype('category').cat.codes\n",
    "train_df['Att26'] = train_df.Att25.astype('category').cat.codes\n",
    "train_df['class'] = train_df['class'].astype('category').cat.codes\n",
    "\n",
    "# Test Data\n",
    "test_df['Att01'] = test_df.Att01.astype('category').cat.codes\n",
    "test_df['Att11'] = test_df.Att11.astype('category').cat.codes\n",
    "test_df['Att12'] = test_df.Att12.astype('category').cat.codes\n",
    "test_df['Att16'] = test_df.Att12.astype('category').cat.codes\n",
    "test_df['Att25'] = test_df.Att12.astype('category').cat.codes\n",
    "test_df['Att26'] = test_df.Att12.astype('category').cat.codes\n",
    "test_df['class'] = test_df['class'].astype('category').cat.codes\n",
    "\n",
    "train_df.dtypes\n",
    "\n",
    "#5)Perform data transformation (such as scaling/standardization):-\n",
    "\n",
    "\n",
    "# some of the arrtibutes do have a large values which neeed to be scaled like att12\n",
    "\n",
    "print(train_df.describe())\n",
    "\n",
    "numeric_att_train = train_df.select_dtypes(include='float').columns\n",
    "numeric_att_test= test_df.select_dtypes(include= \"float\").columns\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_df[numeric_att_train])\n",
    "train_df[numeric_att_train] =  scaler.transform(train_df[numeric_att_train])\n",
    "\n",
    "\n",
    "\n",
    "train_df.describe()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(test_df[numeric_att_test])\n",
    "test_df[numeric_att_test] =  scaler.transform(test_df[numeric_att_test])\n",
    "test_df.columns\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Data Classification \n",
    "\n",
    "\n",
    "trainNA = train_df.isna().sum()\n",
    "trainNA[trainNA != 0]\n",
    "\n",
    "\n",
    "testNA= test_df.isna().sum()\n",
    "testNA[testNA !=0]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import tree\n",
    "\n",
    "y = train_df['class']\n",
    "X = train_df.iloc[:,0:-1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size = 0.25, \n",
    "                                                    random_state = 0) # random_state is a numpy seed to get same values everytime\n",
    "              \n",
    "\n",
    "X_train.shape, X_test.shape\n",
    "\n",
    "#KNN\n",
    "\n",
    "# Create a dictionary of all the parameters we'll be iterating over\n",
    "parameters = {'weights': ['uniform','distance'], # this should be the different weighting schemes\n",
    "              'n_neighbors':[1,3,7,11,17,21]} # this should be a list of the nearest neigbhours\n",
    "# make a classifier object\n",
    "knn = KNeighborsClassifier()\n",
    "# create a GridSearchCV object to do the training with cross validation\n",
    "gscv = GridSearchCV(estimator=knn,\n",
    "                    param_grid=parameters,\n",
    "                    cv=skf,  # the cross validation folding pattern\n",
    "                    scoring='accuracy')\n",
    "# now train our model\n",
    "best_knn = gscv.fit(X_train, y_train)\n",
    "\n",
    "best_knn.best_params_, best_knn.best_score_\n",
    "\n",
    "y_pred = best_Knnmodel.predict(X_test)\n",
    "y_pred\n",
    "\n",
    "print('Overall accuracy of KNN model is:', round(metrics.accuracy_score(y_test, y_pred)*100, 2), '%')\n",
    "\n",
    "#now adding the training dataset into classifieer to make it better performer\n",
    "\n",
    "knn = KNeighborsClassifier(weights = best_knn.best_params_['weights'],\n",
    "                            n_neighbors = best_knn.best_params_['n_neighbors'])\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(6, 6))\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(knn,\n",
    "                                      X_test, y_test,\n",
    "                                      ax=ax)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#Decision Tree\n",
    "\n",
    "\n",
    "# Create a dictionary of all the parameters we'll be iterating over\n",
    "parameters = {'criterion': ('gini','entropy'),  # this should be the different splitting criteria\n",
    "              'min_samples_split':[3,10,15,20]} # this should be the different values for min_samples_split\n",
    "dtc = tree.DecisionTreeClassifier()\n",
    "gscv = GridSearchCV(estimator=dtc,\n",
    "                    param_grid=parameters,\n",
    "                    cv=5,\n",
    "                    scoring='accuracy')\n",
    "best_dtc = gscv.fit(X_train, y_train)\n",
    "best_dtc.best_params_, best_dtc.best_score_\n",
    "\n",
    "dtc = tree.DecisionTreeClassifier(criterion=best_dtc.best_params_['criterion'],\n",
    "                                  min_samples_split=best_dtc.best_params_['min_samples_split'])\n",
    "dtc.fit(X_train, y_train)\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(12,12))\n",
    "tree.plot_tree(dtc,\n",
    "               filled=True, # color the nodes based on class/purity\n",
    "               ax=ax, fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(6, 6))\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(dtc,\n",
    "                                      X_test, y_test,\n",
    "                                      \n",
    "                                      ax=ax)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Naive Bayes Classifier\n",
    "\n",
    "#List of hyperparameters in the dictionary format that we want to tune.\n",
    "parameters = {'var_smoothing': np.logspace(0,-15, num=150)}\n",
    "\n",
    "# Create the Naive-Bayes classifier\n",
    "NB = GaussianNB()\n",
    "\n",
    "# Create GridSearchCV to find the best hyperparameters and do the training\n",
    "gridSearchCV = GridSearchCV(estimator = NB, param_grid = parameters, scoring='accuracy')\n",
    "\n",
    "# now train our model\n",
    "best_NBmodel = gridSearchCV.fit(X_train, y_train)\n",
    "best_NBmodel.best_params_, best_NBmodel.best_score_\n",
    "\n",
    "#Accuracy of Naive Bayes is 72%\n",
    "\n",
    "y_pred = best_NBmodel.predict(X_test)\n",
    "y_pred\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(5, 5))\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(best_NBmodel,\n",
    "                                      X_test, y_test,\n",
    "                                      ax = ax)\n",
    "\n",
    "plt.title('Confusion Matrix for the Naive Bayesian\\n\\n', fontsize = 12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20030611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# was encountering some issue which couldnt be resolve in limited time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
